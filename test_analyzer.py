#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å—Å—ã–ª–æ–∫
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from link_analyzer import WebsiteCrawler
import time

def test_basic_functionality():
    """–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∫—Ä–∞—É–ª–µ—Ä
        crawler = WebsiteCrawler(
            base_url="https://httpbin.org",
            max_pages=2,
            delay=0.5,
            max_workers=2
        )
        
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—É–ª–µ—Ä–∞: OK")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏
        assert crawler.session is not None
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏: OK")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é URL
        test_url = crawler._normalize_url("/test", "https://example.com")
        assert test_url == "https://example.com/test"
        print("‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL: OK")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ–º–µ–Ω–∞
        assert crawler._is_same_domain("https://httpbin.org/test") == True
        assert crawler._is_same_domain("https://google.com") == False
        print("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–∞: OK")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞–∑–æ–≤–æ–º —Ç–µ—Å—Ç–µ: {e}")
        return False

def test_link_extraction():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫"""
    print("\nüîó –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫...")
    
    try:
        crawler = WebsiteCrawler("https://example.com", max_pages=1)
        
        # –¢–µ—Å—Ç–æ–≤—ã–π HTML
        test_html = """
        <html>
        <head>
            <link rel="stylesheet" href="/style.css">
        </head>
        <body>
            <a href="/page1">Internal Link</a>
            <a href="https://external.com">External Link</a>
            <img src="/image.jpg" alt="Test">
            <p>Check out https://textlink.com for more info</p>
        </body>
        </html>
        """
        
        links = crawler.extract_links_from_page("https://example.com", test_html)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Å—ã–ª–∫–∏ –Ω–∞–π–¥–µ–Ω—ã
        assert len(links) > 0
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã —Å—Å—ã–ª–æ–∫
        link_types = [link['link_type'] for link in links]
        assert 'internal' in link_types
        assert 'external' in link_types
        print("‚úÖ –ù–∞–π–¥–µ–Ω—ã –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏: OK")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–µ–≥–∏
        tags = [link['tag'] for link in links]
        assert 'a' in tags
        print("‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ç–µ–≥–æ–≤: OK")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫: {e}")
        return False

def test_link_checking():
    """–¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ —Å—Å—ã–ª–æ–∫"""
    print("\nüåê –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Å—Ç–∞—Ç—É—Å–∞ —Å—Å—ã–ª–æ–∫...")
    
    try:
        crawler = WebsiteCrawler("https://httpbin.org", max_pages=1)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—á—É—é —Å—Å—ã–ª–∫—É
        working_link = {
            'url': 'https://httpbin.org/status/200',
            'link_type': 'external'
        }
        
        result = crawler.check_link_status(working_link)
        
        assert result['url'] == working_link['url']
        assert 'status_code' in result
        assert 'response_time' in result
        print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—á–µ–π —Å—Å—ã–ª–∫–∏: —Å—Ç–∞—Ç—É—Å {result.get('status_code')}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∏—Ç—É—é —Å—Å—ã–ª–∫—É
        broken_link = {
            'url': 'https://httpbin.org/status/404',
            'link_type': 'external'
        }
        
        result = crawler.check_link_status(broken_link)
        assert result['status_code'] == 404
        print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∏—Ç–æ–π —Å—Å—ã–ª–∫–∏: —Å—Ç–∞—Ç—É—Å {result.get('status_code')}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫: {e}")
        return False

def test_real_website():
    """–¢–µ—Å—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ"""
    print("\nüöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ...")
    
    try:
        crawler = WebsiteCrawler(
            base_url="https://httpbin.org",
            max_pages=3,
            delay=0.5,
            max_workers=2
        )
        
        print("–ó–∞–ø—É—Å–∫–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞...")
        start_time = time.time()
        
        crawler.crawl_website()
        
        end_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        assert len(crawler.visited_pages) > 0
        print(f"‚úÖ –ü–æ—Å–µ—â–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(crawler.visited_pages)}")
        
        total_links = sum(len(links) for links in crawler.found_links.values())
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {total_links}")
        
        if total_links > 0:
            print("–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫...")
            crawler.check_all_links()
            print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(crawler.link_status)}")
            
            if crawler.broken_links:
                print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –±–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫: {len(crawler.broken_links)}")
            else:
                print("‚úÖ –ë–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        print(f"‚úÖ –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üî¨ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï WEBSITE LINK ANALYZER")
    print("=" * 50)
    
    all_tests_passed = True
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    tests = [
        test_basic_functionality,
        test_link_extraction,
        test_link_checking,
        test_real_website
    ]
    
    for test_func in tests:
        try:
            if not test_func():
                all_tests_passed = False
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ {test_func.__name__}: {e}")
            all_tests_passed = False
    
    print("\n" + "=" * 50)
    if all_tests_passed:
        print("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–®–õ–ò –£–°–ü–ï–®–ù–û!")
        print("‚úÖ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Å—ã–ª–æ–∫ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
    else:
        print("‚ùå –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ù–ï –ü–†–û–®–õ–ò")
        print("‚ö†Ô∏è  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
    
    return all_tests_passed

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)