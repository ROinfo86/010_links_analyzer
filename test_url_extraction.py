#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL —Å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from link_analyzer import WebsiteCrawler

def test_url_extraction_with_encoding():
    """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL —Å –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    print("üîó –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ URL —Å %-encoding...")
    
    crawler = WebsiteCrawler("https://example.com", max_pages=1)
    
    # HTML —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ URL –≤–∫–ª—é—á–∞—è %-encoded
    test_html = """
    <html>
    <body>
        <p>–û–±—ã—á–Ω–∞—è —Å—Å—ã–ª–∫–∞: https://example.com/page</p>
        <p>–°—Å—ã–ª–∫–∞ —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏: https://example.com/search?q=hello%20world%26more</p>
        <p>–°–ª–æ–∂–Ω–∞—è —Å—Å—ã–ª–∫–∞: https://api.example.com/v1/users?filter=%7B%22name%22%3A%22John%22%7D&sort=asc</p>
        <p>–°—Å—ã–ª–∫–∞ —Å —è–∫–æ—Ä–µ–º: https://docs.example.com/guide#section%20title</p>
        <p>–°—Å—ã–ª–∫–∞ —Å –ø–æ—Ä—Ç–æ–º: https://localhost:8080/api/v1/data?param=%2Fpath%2Fto%2Ffile</p>
        <a href="https://encoded.example.com/path%20with%20spaces">Link with spaces</a>
    </body>
    </html>
    """
    
    links = crawler.extract_links_from_page("https://example.com/test", test_html)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é —Å—Å—ã–ª–∫—É
    found_urls = [link['url'] for link in links]
    
    expected_patterns = [
        '%20',  # –ø—Ä–æ–±–µ–ª
        '%26',  # –∞–º–ø–µ—Ä—Å–∞–Ω–¥  
        '%7B',  # {
        '%22',  # "
        '%3A',  # :
        '%7D',  # }
        '%2F',  # /
    ]
    
    print("\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ URL:")
    for i, url in enumerate(found_urls, 1):
        print(f"{i}. {url}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ %-encoded —Å–∏–º–≤–æ–ª–æ–≤
        has_encoding = any(pattern in url for pattern in expected_patterns)
        if has_encoding:
            print(f"   ‚úÖ –°–æ–¥–µ—Ä–∂–∏—Ç URL-encoding")
        
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–∞–π–¥–µ–Ω—ã URL —Å %-encoding
    urls_with_encoding = [url for url in found_urls if '%' in url]
    
    print(f"\nURL —Å %-encoding: {len(urls_with_encoding)}")
    for url in urls_with_encoding:
        print(f"  - {url}")
    
    if len(urls_with_encoding) > 0:
        print("‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω: URL —Å %-encoding –Ω–∞–π–¥–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
    else:
        print("‚ùå –¢–µ—Å—Ç –ø—Ä–æ–≤–∞–ª–µ–Ω: URL —Å %-encoding –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return False

def test_edge_cases():
    """–¢–µ—Å—Ç –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤"""
    print("\nüéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏...")
    
    crawler = WebsiteCrawler("https://example.com", max_pages=1)
    
    edge_case_html = """
    <html>
    <body>
        <p>URL –≤ —Å–∫–æ–±–∫–∞—Ö: (https://example.com/bracketed%20url)</p>
        <p>URL –≤ –∫–∞–≤—ã—á–∫–∞—Ö: "https://example.com/quoted%20url"</p>
        <p>URL –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –°–º–æ—Ç—Ä–∏—Ç–µ https://example.com/end%20url.</p>
        <p>–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π URL: https://verylongdomain.example.com/very/long/path/with/many/segments?param1=value1%20with%20spaces&param2=value2%26encoded&param3=%7B%22json%22%3A%22data%22%7D#section%20with%20spaces</p>
        <p>URL —Å —Ä—É—Å—Å–∫–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏ –≤ %-encoding: https://example.com/–ø–æ–∏—Å–∫ -> https://example.com/%D0%BF%D0%BE%D0%B8%D1%81%D0%BA</p>
    </body>
    </html>
    """
    
    links = crawler.extract_links_from_page("https://example.com/edge", edge_case_html)
    
    print(f"–ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –≤ –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö: {len(links)}")
    
    for i, link in enumerate(links, 1):
        url = link['url']
        print(f"{i}. {url}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL –Ω–µ –æ–±—Ä–µ–∑–∞–µ—Ç—Å—è –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–∞—Ö
        if url.endswith('.') or url.endswith(')') or url.endswith('"'):
            print(f"   ‚ö†Ô∏è  –í–æ–∑–º–æ–∂–Ω–æ –æ–±—Ä–µ–∑–∞–Ω –Ω–∞: {url[-1]}")
        else:
            print(f"   ‚úÖ –í—ã–≥–ª—è–¥–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    
    return True

if __name__ == "__main__":
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï URL-EXTRACTION –° –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ú–ò –°–ò–ú–í–û–õ–ê–ú–ò")
    print("=" * 60)
    
    try:
        test1_passed = test_url_extraction_with_encoding()
        test2_passed = test_edge_cases()
        
        print("\n" + "=" * 60)
        if test1_passed and test2_passed:
            print("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!")
            print("‚úÖ URL —Å %-encoding —Ç–µ–ø–µ—Ä—å –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print("‚ùå –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ù–ï –ü–†–û–®–õ–ò")
            
    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–∞—Ö: {e}")
        import traceback
        traceback.print_exc()